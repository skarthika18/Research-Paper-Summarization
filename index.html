<html><head>
    <link rel="stylesheet" href="index.css">
    <script src="index.js"></script>
    <script src="particles.js"></script>
    <script src="particles-config.js"></script>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.0/css/bootstrap.min.css">
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.0/js/bootstrap.min.js"></script>
</head>
<body>
<div id="userinfodiv" class="">
    <div id="particles-js">
    </div>
    <div id="userinfobox"><div id="userinfo">
    <a href="https://www.linkedin.com/in/shyamsubramanian/" target="_blank"><img src="images/WPI.jpg" id="profilepic"></a>
    
    
</div><span id="abbr" style="font-size:30px; margin-top:30px;">RPIAN</span></span><div id="info"></div></div>
<div id="Bio"><div id="biotext"><h1 class="header">Research Papers In a nutshell</h1><h2>IR Final Project</h2><div id="name">Shyam Subramanian, Rashmi Singh, Abhishek Shah, Karthika Suresh</div>
</div> 
<div id="identifier">
<div class="identifier" id="edu">PROJECT PROPOSAL</div>
<div class="identifier" id="cou">ISSUES TO ADDRESS</div>
<div class="identifier" id="ski">SOLUTION FLOWCHART</div>
<div class="identifier" id="exp">EXPERIMENTS</div>
<div class="identifier" id="aca">DATASET</div>
<div class="identifier" id="con">RESULTS</div></div></div></div>


<div id="dummydiv"></div>
<div>
    <h1 style="color:black;">What we are trying to do?</h1>
    <p style="color:black; font-size:20px;">Summarize research papers by either extractive or abstractive methods to capture important keypoints from them.</p>
</div>
<div id="resume-start"><div id="education" class="tabcontent">
<div id="edutitle" class="tabtitle collapsible active">PROJECT PROPOSAL<div class="expand"></div></div>
<div id="edutext" style="max-height: 125px;" class="textcontent">
    <div>The function of our tool is to summarize research papers by extracting important sentences from them. The goal here is to find an automated way of quantifying the significance of each sentence so that only most significant sentences are present in the summarized version. The order of sentences in summary needs to be sensible without loss of continuity. We also intend to provide a downloadable PowerPoint of the summarized version of research paper. The created summary would be such that it is easier for people to understand the main idea and its usefulness will find a middle ground between reading only the abstract and reading the entire research paper.</div>
</div>
</div><div id="courses" class="tabcontent">
<div id="coursestitle" class="tabtitle collapsible active">ISSUES TO ADDRESS<div class="expand"></div></div>
<div id="coursestext" style="max-height: 112px;" class="textcontent">
<li>No Ground Truth Summaries for Research papers</li>
<li>Research Papers are Lengthy</li>
<li>Research papers are of different lengths</li>
<li>Proper Transfer Learning</li>
<li>Mathematical Formulas, Images and other unstructured data</li>
</div>
</div><div id="skills" class="tabcontent">
<div id="skillstitle" class="tabtitle collapsible active">SOLUTION FLOWCHART<div class="expand"></div></div>
<div id="skillstext" style="max-height: 600px;" class="textcontent">
    <img src="images/flowchart.png" alt="Flowchart for solution" style="display: block; margin: auto;"></img>
</div>
</div>

<div id="workexp" class="tabcontent">
<div id="workexptitle" class="tabtitle collapsible active">
EXPERIMENTS
<div class="expand"></div>
</div>    
<div id="workexptext" style="max-height: 2000px;" class="textcontent">
<h4><u>EXTRACTIVE SUMMARIZATION</u></h4>
<div id="textrankalg">
<span style="display:block"><u>TEXTRANK ALGORITHM</u></span>
<br/>
<img src="images/page_rank.png" style="display: block; margin: auto;"></img>
</div>
<br/>
<div id="lsaalg">
<span style="display:block"><u>LATENT SEMANTIC ANALYSIS ALGORITHM</u></span>
<br/>
<img src="images/lsa.png" style="display: block; margin: auto;"></img>
</div>
<br/>
<div id="luhnalg">
<span style="display:block"><u>LUHN's ALGORITHM</u></span>
<br/>
<img src="images/luhn.png" style="display: block; margin: auto;"></img>
</div>
<br/><br/>
<h4><u>ABSTRACTIVE SUMMARIZATION</u></h4>
<div id="pgnalg">
<span style="display:block"><u>POINTER GENERATOR ALGORITHM</u></span>
<br/>
<img src="images/pgn.png" style="display: block; margin: auto;"></img>
</div>

</div>
</div>


<div id="data" class="tabcontent">
    <div id="contacttitle" class="tabtitle collapsible active">DATASET<div class="expand"></div></div>
    <div id="contacttext" style="max-height: 112px;" class="textcontent"><li><a href="https://github.com/mahnazkoupaee/WikiHow-Dataset">WikiHow</a></li><li><a href="https://github.com/WING-NUS/scisumm-corpus">SciSumm</a></li></div>
</div>


<div id="projects" class="tabcontent">
    <div id="projectstitle" class="tabtitle collapsible active">RESULTS<div class="expand"></div></div>
    <div id="projectstext" style="max-height: 1500px;" class="textcontent">
    <table>
    <tr>
        <td>
            <button type="button" class="btn btn-info" data-toggle="collapse" data-target="#demo">Paper</button>
            <div id="demo" class="collapse"><font color="black"><a href="https://github.com/WING-NUS/scisumm-corpus/tree/master/data/Training-Set-2018/C00-2123/Citance_XML">PDF</a></font></div>
        </td>
        <td>
                &nbsp;
                <button type="button" class="btn btn-info" data-toggle="collapse" data-target="#demoa">Abstract</button>
                <div id="demoa" class="collapse"><font color="black">
            The statistical approach to machine translation regards the machine translation problem as the maximum likelihood solution of a translation target text given a translation source text.
        According to the Bayes Rule, the problem is transformed into the noisy channel model paradigm, where the translation is the maximum a posteriori solution of a distribution for a channel target text given a channel source text and a prior distribution for the channel source text (Brown et al., 1993).
        Although there exists efficient algorithms to estimate the parameters for the statistical machine translation (SMT), one of the problems of SMT is the search algorithms for the translation given a sequence of words.
        There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.
        The algorithms proposed above cannot deal with drastically different word correspondence, such as Japanese and English translation, where Japanese is SOV while SVO in English.
        Germann et al.
        (2001) suggested greedy method and integer programming decoding, though the first method suffer from the similar problem as described above and the second is impractical for the real-world application.
        This paper presents two decoding methods, one is the right-to-left decoding based on the left-to- right beam search algorithm, which generates outputs from the end of a sentence.
        The second one is the bidirectional decoding method which decodes in both of the left-to-right and right-to-left directions and merges the two hypothesized partial sentences into one.
        The experimental results of Japanese and English translation indicated that the right-to-left decoding was better for English-to-Japanese translation, while the left-to-right decoding was better for Japanese-to-English decoding.
        The above results could be justified by the structural difference of Japanese and English, where English takes the prefix structure that places emphasis at the beginning of a sentence, hence prefers left-to-right decoding.
        On the other hand, Japanese takes postfix structure, setting attention around the end of a sentence, therefore favors right-to-left decoding.
        The bidirectional decoding, which can take both of the benefits of decoding method, was superior to mono- directional decoding methods.
        The next section briefly describes the SMT focusing on the IBM Model 4.
        Then, the Section 3 presents decoding algorithms in three direction, left- to-right, right-to-left and bi-direction.
        The Section 4 presents the results of Japanese and English translation followed by discussions.
        </font>
            </div>
        </td>
        
        <td>&nbsp;
        <button type="button" class="btn btn-info" data-toggle="collapse" data-target="#demo1">TextRank</button>
        <div id="demo1" class="collapse"><font color="black">
    This paper presents two decoding methods, one is the right-to-left decoding based on the left-to- right beam search algorithm, which generates outputs from the end of a sentence.
Then, the Section 3 presents decoding algorithms in three direction, left- to-right, right-to-left and bi-direction.
Statistical machine translation regards machine translation as a process of translating a source lan NULL0 could1 you2 recommend3 another4 hotel5 hoka no hoteru o shokaishi teitadake masu ka a = (4, 4, 5, 0, 3, 1, 1, 0) Figure 1: An example of alignment for Japanese and English sentences guage text (f) into a target language text (e) with the following formula: e = arg max P(e f) e The Bayes Rule is applied to the above to derive: e = arg max P(f e)P(e) e The translation process is treated as a noisy channel model, like those used in speech recognition in which there exists e transcribed as f, and a translation is to infer the best e from f in terms of P(f|e)P(e).
The algorithm assumes two kinds of partial hypotheses2, translated partially from an input string, one is an open hypothesis that can be extended by raising the fertility.
Table 2 summarizes the results of decoding by left-to-right, right-to-left and bidirectional method evaluated with WER, PER and SE.
Table 3 shows the ratio of producing search errors, computed by comparing the translation model and lnguage model scores for the outputs from three decoding methods.
Table 2: Summary of results for Japanese-to-English (J-E) and English-to-Japanese (E-J) translations by left-to-right (LtoR), right-to-left (RtoL) and bidirectional (Bi) decoding methods.
</font>
    </div>
    </td><td>&nbsp;
    <button type="button" class="btn btn-info" data-toggle="collapse" data-target="#demo2">LSA</button>
        <div id="demo2" class="collapse"><font color="black">
    There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.
On the other hand, Japanese takes postfix structure, setting attention around the end of a sentence, therefore favors right-to-left decoding.
The search problem of statistical machine translation is to induce the maximum likely channel source sequence, e, given f and the model, P(f|e) =La P(f, a|e) and P(e).
The skipping based criteria, such as introduced by Och et al.
(2001), is not appropriate for the language pairs with drastically different alignment, such as Japanese and English, hence was not considered in this paper.
The decoding algorithm generating in left-to-right direction fills the output sequence from the beginning of a sentence by consuming the input words in any order and by selecting the corresponding translation.
Table 2 summarizes the results of decoding by left-to-right, right-to-left and bidirectional method evaluated with WER, PER and SE.
The PER is the one similar to WER but ignores the positions, allowing the reordered outputs, hence can estimate the accuracy for the tranlslation word selection.
0 57 .6 56 .1 58 .0 49 .3 % 10.0% 8.7% 32.0% 49 .3 % 10.0% 6.7% 34.0% 48 .7 % 8.0% 10.0% 33.3% input: suri ni saifu o sura re mashi ta (i had my pocket picked) LtoR: here ’s my wallet was stolen RtoL: here ’s my wallet was stolen Bi: i had my wallet stolen input: sumimasen ga terasu no seki ga ii no desu ga (excuse me but can we have a table on the terrace) LtoR: excuse me i ’d like a seat on the terrace RtoL: i ’d prefer excuse me Bi: i ’d like a seat on the terrace input: nan ji ni owaru no desu (what time will it be over) LtoR: what time should i be at the end RtoL: it ’s what time will it be over Bi : at what time is it end input: nimotsu o ue ni age te morae masu ka (will you put my luggage on the rack) LtoR: could you put my baggage here RtoL: do you have overhead luggage Bi: could you put my baggage input: ee ani to imouto ga hitori zutsu i masu (yes i have a brother and a sister) LtoR: yes brother and sister there a daughter RtoL: you ’re yes brother and sister daughter Bi: yes my daughter is there a brother and sister Figure 6: Examples of Japanese-to-English translation guage model employed for this experiment, for the language model probabilities were assigned based on the left history, not the right history.
</font>
    </div></td>
    <td>&nbsp;
    <button type="button" class="btn btn-info" data-toggle="collapse" data-target="#demo3">LUNH</button>
        <div id="demo3" class="collapse"><font color="black">
    The above results could be justified by the structural difference of Japanese and English, where English takes the prefix structure that places emphasis at the beginning of a sentence, hence prefers left-to-right decoding.
On the other hand, Japanese takes postfix structure, setting attention around the end of a sentence, therefore favors right-to-left decoding.
The former term, P(f|e), is a translation model representing some correspondence between bilingual text.
This problem is known to be NP-Complete (Knight, 1999), for the reordering property in the model further complicates the search.
Again, the right-to-left direction is suitable for the language which enforces stronger constraints at the end of sentence, such as Japanese, similar to the reason mentioned above.
The similar statement can hold for postfix languages, such as Japanese, where emphasis is placed around the end of a sentence.
The translation models, both for the Japanese-to- English (J-E) and English-to-Japanese (E-J) translation, were trained toward IBM Model 4 on the training set and cross-validated on validation set to terminate the iteration by observing perplexity.
The translation was carried out by three decoding methods:left-to-right, right-to- left and bidirectional one.
The PER is the one similar to WER but ignores the positions, allowing the reordered outputs, hence can estimate the accuracy for the tranlslation word selection.
0 57 .6 56 .1 58 .0 49 .3 % 10.0% 8.7% 32.0% 49 .3 % 10.0% 6.7% 34.0% 48 .7 % 8.0% 10.0% 33.3% input: suri ni saifu o sura re mashi ta (i had my pocket picked) LtoR: here ’s my wallet was stolen RtoL: here ’s my wallet was stolen Bi: i had my wallet stolen input: sumimasen ga terasu no seki ga ii no desu ga (excuse me but can we have a table on the terrace) LtoR: excuse me i ’d like a seat on the terrace RtoL: i ’d prefer excuse me Bi: i ’d like a seat on the terrace input: nan ji ni owaru no desu (what time will it be over) LtoR: what time should i be at the end RtoL: it ’s what time will it be over Bi : at what time is it end input: nimotsu o ue ni age te morae masu ka (will you put my luggage on the rack) LtoR: could you put my baggage here RtoL: do you have overhead luggage Bi: could you put my baggage input: ee ani to imouto ga hitori zutsu i masu (yes i have a brother and a sister) LtoR: yes brother and sister there a daughter RtoL: you ’re yes brother and sister daughter Bi: yes my daughter is there a brother and sister Figure 6: Examples of Japanese-to-English translation guage model employed for this experiment, for the language model probabilities were assigned based on the left history, not the right history.
</font>
    </div></td>
    <td>&nbsp;
    <button type="button" class="btn btn-info" data-toggle="collapse" data-target="#demo4">PGN</button>
        <div id="demo4" class="collapse"><font color="black">
    where english takes the prefix structure that places emphasis it the second one is the right-to-left decoding method which decodes in both of the left-to-right and right-to-left directions and merges the translation source and target texts .
the above results could be justified by the structural difference of japanese and english , where english takes the prefix structure , setting attention around the end of a sentence .
then , the section 3 presents decoding algorithms in three direction , left - to-right and bi-direction .
the search problem of statistical machine translation is to induce the maximum likely channel model , 5 , 0 , 3 , 1 , 0 -rrb- figure 1 : an example of alignment for japanese and english sentences guage text -lrb- f -rrb- into a target language text -lrb- e -rrb- e the translation process is treated as a noisy channel model , like those used in speech recognition in which there exists e transcribed as f , and a translation is to the previous target word generated from the cept -lrb- j -rrb- .
the position of a non-head
the decoding methods presented in this paper explore the partial candidate translation hypotheses greedily , as presented in tillmann and ney -lrb- 2000 -rrb- and och et al. .
the algorithm assumes two kinds of partial hypotheses2 , translated partially from an input string , one is open hypothesis that can be extended by raising the fertility .
the decoding algorithm generating in left-to-right direction fills the output sequence from the beginning of a sentence by consuming the input words in any order and by selecting the corresponding translation it
sample japanese-to-english translations performed by comparing the translation model and lnguage model scores for the outputs from three decoding methods .
table 3 shows the ratio of producing search errors , right-to-left and bidirectional method evaluated with wer , per and se .
table 2 summarizes the results of decoding by left-to-right , right-to-left and bidirectional method evaluated with wer , 2002 -rrb- .
the translation was carried out by three decoding methods : left-to-right , right-to - left and bidirectional .
sample japanese-to-english translations performed by the decoders is presented in figure 6 .
the left-to-right decoding method performed better than the right-to-left one in terms of wer/per scores , though the se score dropped from 8.7 % to 6.7 % in cranked sentences .
table 2 : summary of results for japanese-to-english -lrb- j-e -rrb- translations by left-to-right -lrb- ltor -rrb- , right-to-left -lrb- rtol -rrb- and bidirectional -lrb- bi -rrb- decoding methods .
nevertheless , the search error decreased from 59.3 into 34.0 by alternating the translation direction for the right-to-left decoding method , which still supports the use of correct rendering direction for translation target language .
</font>
    </div></td>
    </tr>
    
    <tr>
        <td>
    <button type="button" class="btn btn-info" data-toggle="collapse" data-target="#demo33">Paper</button>
        <div id="demo33" class="collapse"><font color="black">
                <a href="https://github.com/WING-NUS/scisumm-corpus/tree/master/data/Training-Set-2018/C00-2123/Citance_XML">PDF</a>
    </font>
    </div></td>
    <td>&nbsp;
        <button type="button" class="btn btn-info" data-toggle="collapse" data-target="#demoa3">Abstract</button>
        <div id="demoa3" class="collapse"><font color="black">
    Decoding is one of the three fundamental problems in classical SMT (translation model and language model being the other two) as proposed by IBM in the early 1990’s (Brown et al., 1993).
In the decoding problem we are given the language and translation models and a source language sentence and are asked to find themost probable translation for the sentence.
De coding is a discrete optimization problem whose search space is prohibitively large.
The challenge is, therefore, in devising a scheme to efficiently search the solution space for the solution.
Decoding is known to belong to a class of computational problems popularly known as NP- hard problems (Knight, 1999).
NP-hard problems are known to be computationally hard and have eluded polynomial time algorithms (Garey and Johnson, 1979).
The first algorithms for the decoding problem were based on what is known among the speech recognition community as stack-based search (Jelinek, 1969).
The original IBM solution to the decoding problem employed a restricted stack-based search (Berger et al., 1996).
This idea was further explored by Wang and Waibel (Wang and Waibel, 1997) who developed a faster stack-based search algorithm.
In perhaps the first work on the computational complexity of Decoding, Kevin Knight showed that the problem is closely related to the more famous Traveling Salesman problem (TSP).
Independently, Christoph Tillman adapted the Held-Karp dynamic programming algorithm for TSP (Held and Karp, 1962) to Decoding (Tillman, 2001).
The original Held- Karp algorithm for TSP is an exponential time dynamic programming algorithm and Tillman’s adaptation to Decoding has a prohibitive com plexity of O (l3m2 2m ) ≈ O (m5 2m ) (where mand l are the lengths of the source and tar get sentences respectively).
Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave a O (l3m4) ≈ O (m7) algo rithm for French-English translation (Tillman and Ney, 2000).
An optimal decoder based on the well-known A∗ heuristic was implemented and benchmarked in (Och et al., 2001).
Since optimal solution can not be computed for practical problem instances in a reasonable amount of time, much of recent work has focused on good quality suboptimal solutions.
An O (m6) greedy search algorithm was developed (Germann et al., 2003) whose complexity was re duced further to O (m2) (Germann, 2003).
In this paper, we propose an algorithmic framework for solving the decoding problem and show that several efficient decoding algorithms can be derived from the techniques developed in the framework.
We model the search problem as an alternating search problem.
The search, therefore, alternates between two subproblems, both of which are much easier to solve in practice.
By breaking the decoding problem into two simpler search problems, we are able to provide handles for solving the problem efficiently.
The solutions of the subproblems can be combined easily to arrive at a solution for the original problem.
The first subproblem fixes an alignment and seeks the best translation with that alignment.
Starting with an initial alignment between the source sentence and its translation, the second subproblem asks for an improved alignment.
We show that both of these problems are easy to solve and provide efficient solutions for them.
In an iterative search for a local optimal solution, we alternate between the two algorithms and refine our solution.
The algorithmic framework provides handles for solving the decoding problem at several levels of complexity.
At one extreme, the framework yields an algorithm for solving the decoding problem optimally.
At the other extreme, it yields a provably linear time algorithm for finding suboptimal solutions to the problem.
We show that the algorithmic handles provided by our framework can be employed to develop a very fast decoding algorithm which finds good quality translations.
Our fast suboptimal search algorithms can translate sentences that are 50 words long in about 5 seconds on a simple computing facility.
The rest of the paper is devoted to the development and discussion of our framework.
We start with a mathematical formulation of the decoding problem (Section 2).
We then develop the alternating search paradigm and use it to develop several decoding algorithms (Section 3).
Next, we demonstrate the practical utility of our algorithms with the help of results from our initial experiments (Section 5).

</font>
    </div>
    </td>
        <td>&nbsp;
        <button type="button" class="btn btn-info" data-toggle="collapse" data-target="#demo73">TextRank</button>
        <div id="demo73" class="collapse"><font color="black">
    The algorithmic framework provides handles for solving the decoding problem at several levels of complexity.
We show that the algorithmic handles provided by our framework can be employed to develop a very fast decoding algorithm which finds good quality translations.
The decoding problem in SMT is one of finding Rewriting the translation model P r(f |e) as �a P r(f , a|e), where a denotes an alignmentbetween the source sentence and the target sen tence, the problem can be restated as: eˆ = argmaxe ) P r(f , a|e)P r(e).
While RELAXED DECODING is simpler than STRICT DECODING, it is also, unfortunately, NP hard for even IBM Model 1 and Bigram language model.
In the remainder of this paper, we use the term Viterbi to denote any linear time algorithm for computing an improved alignment between the source sentence and its translation.
Recall that in FIXED ALIGNMENT DECODING, we are given the target length l and a mapping a˜ from source words to target positions.
each length class.
length class.
Plot 1 shows the average time taken by the algorithms for translating the sentences in each length class.
Answers to these questions will result in faster and more efficient decoding algorithms.
</font>
    </div>
    </td><td>&nbsp;
    <button type="button" class="btn btn-info" data-toggle="collapse" data-target="#demo38">LSA</button>
        <div id="demo38" class="collapse"><font color="black">
    At the other extreme, it yields a provably linear time algorithm for finding suboptimal solutions to the problem.
Our fast suboptimal search algorithms can translate sentences that are 50 words long in about 5 seconds on a simple computing facility.
Since the Fundamental Equation of SMT does not yield an easy handle to design a solution (exact or even an approximate one) for the problem, most researchers have instead worked on solving the following relatively simpler problem (Germann et al., 2003): (eˆ, aˆ) = argmax(e,a) P r(f , a|e)P r(e).
The challenge is in devising fast search strategies to find good suboptimal solutions.
As we will show later, the optimal solution for FIXED ALIGNMENT DECODING can be computed in O (m) time for IBM models 15 using dynamic programming.
In the remainder of this paper, we use the term Viterbi to denote any linear time algorithm for computing an improved alignment between the source sentence and its translation.
In this section, we give a dynamic programming based solution to this problem.
, el that maximizes the LHS of the above optimization function can be found in O (m) time using the standard Dynamic Programming algorithm (Cormen et al., 2001).
We implemented the algorithms in C++ and conducted the experiments on an IBM RS-6000 dual processor machine with 1 GB of RAM.
We built an English language model by training over a corpus consisting of about 800 million words.
U. Ger man n, M. Jahr , D. Mar cu, and K. Ya010 1120 2130 3140 4150 51 Sentence Length Figure 2: NIST scores Logscores mada.
A fa st se q ue nt ia l de co di n g al - go ri th m us in g a st ac k. I B M J o ur n al R es ea ch a n d D ev el o p m e nt , 1 3: 6 7 5 – 6 8 5.
</font>
    </div></td>
    <td>&nbsp;
    <button type="button" class="btn btn-info" data-toggle="collapse" data-target="#demo34">LUNH</button>
        <div id="demo34" class="collapse"><font color="black">
    The first algorithms for the decoding problem were based on what is known among the speech recognition community as stack-based search (Jelinek, 1969).
Our fast suboptimal search algorithms can translate sentences that are 50 words long in about 5 seconds on a simple computing facility.
Unless P = NP, there is no hope of an efficient algorithm for the decoding problem.
Since the Fundamental Equation of SMT does not yield an easy handle to design a solution (exact or even an approximate one) for the problem, most researchers have instead worked on solving the following relatively simpler problem (Germann et al., 2003): (eˆ, aˆ) = argmax(e,a) P r(f , a|e)P r(e).
The first of these observations concerns with solving the problem when we know in advance the mapping between the source and target sentences.
In the remainder of this paper, we use the term Viterbi to denote any linear time algorithm for computing an improved alignment between the source sentence and its translation.
Note that each word ei has only a constant number of candidates in the vocabulary.
, el that maximizes the LHS of the above optimization function can be found in O (m) time using the standard Dynamic Programming algorithm (Cormen et al., 2001).
We implemented the algorithms in C++ and conducted the experiments on an IBM RS-6000 dual processor machine with 1 GB of RAM.
We built an English language model by training over a corpus consisting of about 800 million words.
U. Ger man n, M. Jahr , D. Mar cu, and K. Ya010 1120 2130 3140 4150 51 Sentence Length Figure 2: NIST scores Logscores mada.
We have also shown that alternating maximization can be employed to come up with O (m2) decoding algorithm.
</font>
    </div></td>
    <td>&nbsp;
    <button type="button" class="btn btn-info" data-toggle="collapse" data-target="#demo43">PGN</button>
        <div id="demo43" class="collapse"><font color="black">
    employed a restricted stack-based search -lrb- berger et al. , 1996 -rrb- .
since optimal solution can not be computed for practical problem instances in a reasonable amount of time , much of recent work has focused on good quality suboptimal solutions .
the algorithmic framework provides handles for solving the decoding problem optimally .
where a denotes an alignmentbetween the source sentence and the target sen tence , the problem can be restated as : eˆ = argmaxe -rrb- p r -lrb- f , a | e -rrb- .
-lrb- 2 -rrb- a even when the translation model p r -lrb- f , a | e -rrb- .
-lrb- 3 -rrb- we call the search problem specified by equation 3 as relaxed decoding .
therefore , all practical solutions to relaxed decoding have focused on finding suboptimal solutions .
the optimal translation for the source sentence f , with the target sen tence and the alignment -rrb- can be computed efficiently .
in the remainder of this paper , we use the term viterbi to denote any linear time algorithm for computing an improved alignment between the source sentence and its translation at then com putes the optimal translation for the source andthat computes a suboptimal solution for re laxed decoding while naiveoptimaldecode is an exponential at
above reformulation of the optimization function of decoding problem allows us to employ dynamic programming for solving fixed alignment decoding efficiently .
the above reformulation of the optimization function of the decoding problem allows us to employ dynamic programming for solving fixed alignment decoding .
300 test french sentences in each length class .
plot 3 shows that the log scores of translations computed by our algorithms are very close to those computed by the held-karp algorithm it each length class .
plot 1 shows the average time taken by the algorithms for translating the sentences in each length class .
that alternating maximization can be employed to come up with o -lrb- m2 .
separately , is it possible to explore a bunch of alignments in one shot ? answers to these questions will result in faster and more efficient decoding algorithm .
we have also shown that alternating maximization can be employed to come o -lrb- m2 -rrb- decoding algorithm it
</font>
    </div></td>
    </tr>
    <tr>
        <td>
    <button type="button" class="btn btn-info" data-toggle="collapse" data-target="#demo9">Paper</button>
        <div id="demo9" class="collapse">
            <font color="black">
        
                    
                    <a href = "https://github.com/WING-NUS/scisumm-corpus/tree/master/data/Training-Set-2018/C00-2123/Citance_XML">PDF</a>
    </font>
    </div></td>
    <td>&nbsp;
        <button type="button" class="btn btn-info" data-toggle="collapse" data-target="#demo9a">Abstract</button>
        <div id="demo9a" class="collapse"><font color="black">
    In this paper, we address the question of how part­ of-speech ( POS) information can help improv­ ing the quality of Statistical Machine Translation (SMT).
One of the main problems when translat­ ing from a language with hm·dly any inflectional morphology (which is English in our experiments) into one with richer morphology (here: Spanish and Catalan) is the production of the correct in­ flected form in the target lan guage.
We introduce transformations to the English string that are based on the part-of-speech information and show how this knowledge source can help SMT.
Systematic evaluations will show that the quality of the gen erated translations is improved.
The transfonnations we apply arc the following: Treatment of verbs In Catalan and Spanish, the pronoun before a verb is often omitted and in­ stead, the person is expressed via the ending of the verb.
The smne holds for future tense and for the modes expressed through 'would ' and 'should' in English.
Since this makes it hard to generate the correct translation of a given English verb, we propose a method re­ sulting in English word forms containing suf­ ficient infonnation.
Question inversion In English, interrogative phrases have a word order that is different from declarative sentences: Either an auxil­ iary 'do·is inserted or the order of verb and pronoun is inverted.
Since this is dilTerent in Spanish and Catalan, we modify the word order in English to make it more similar to the Spanish/Catalan one and to help the verb treatment mentioned above.
The paper is organized as follows: Related work is treated in Section 2.
In Section 3, we shortly review the statistical approach to machine transla­ tion.
Then, we introduce the transfonnations that we apply to the less inflected language of the two under consideration (namely English) in Section 4.
After describing the maximum entropy approach and the training procedure we use for the statisti­ cal lexicon in Section 5, we present results on the trilingual LC-STAR corpus in Section 6.
Then, we conclude and present ideas about future work in Section 7.
</font>
    </div>
    </td>
        <td>&nbsp;
        <button type="button" class="btn btn-info" data-toggle="collapse" data-target="#demo19">TextRank</button>
        <div id="demo19" class="collapse"><font color="black">
    In this paper, we address the question of how part­ of-speech ( POS) information can help improv­ ing the quality of Statistical Machine Translation (SMT).
The transfonnations we apply arc the following: Treatment of verbs In Catalan and Spanish, the pronoun before a verb is often omitted and in­ stead, the person is expressed via the ending of the verb.
(NieBen and Ney, 200 Ib) introduce hier­ archical lexicon models including baseform and POS information for translation from German into English.
Information contained in the German en­ tries that are not relevant for the generation of the English translation arc omitted.
Applying Bayes' deci­ sion rule yields the following criterion: arg max Pr ( t{ [s{ ) tf = arg m x{Pr( l{) · Pr( s{l l{)} (1) tl Through this decomposition of the probability, we obtain two knowledge sources: the translation and the language model.
The search is denoted by the arg rna.x operation in Eq. 1, i.e. it explores the space of all possible target language strings and all possible alignments between the source and the target language string to find the one with maximal probability.
As the last example in Table l shows, 'you can go' is spliced only into two words instead of one in order to better match the Spanish/Catalan form.
The au x iliary 'do' does not carry information that is relevant when translating into Table 1: Examples of spliced words in the English vocabulary I original I POS tags I spliced words I yo u go P R P V B P yo u_ go yo u we nt PR P V B D yo u we nt yo u thi nk PR P V BP yo u_ thi nk yo u wi ll ha ve P R P M D V B yo u wi ll ha ve yo u ca n go P R P M D V B yo u ca n go Spanish or Catalan.
Training If we merge the pronouns/modals and verbs as de­ scribed above, it might happen thalthe verh itsell' (or one of its inflections) has never been seen in training except fi·om its appearance in the new en­ tries in the lexicon which result from the splic Table 2: Examples of spliced words in the English vocabulary after question inversion ori gi na l PO S ta gs spl ice d w or ds do yo u go V BP PR P V B yo u_ go di d yo u go V B D P R P V B yo u_ di d go ha ve yo u go ne V B P P R P V B N yo u_ ha ve go ne wi ll yo u go M D P R P V B yo u_ wi lL go ca n yo u go P R P M D V B yo u_ ca n go Table 3: Examples of transformed English sentences Or igi na l ho w are yo u?
We performed the following training steps: • transform the English (= source language) part or the corpus as described in Seclions 4.1 and 4.2 • train the statistical translation system using this modi fled source language corpus 1 • with the resuiLing alignment, train the lexicon model using maximum entropy with the fea­ tures described in Section 5.1 This training can be pelformed using converg­ ing iterative training procedures like described by (Darroch and Ratcliff.
</font>
    </div>
    </td><td>&nbsp;
    <button type="button" class="btn btn-info" data-toggle="collapse" data-target="#demo29">LSA</button>
        <div id="demo29" class="collapse"><font color="black">
    In this paper, we address the question of how part­ of-speech ( POS) information can help improv­ ing the quality of Statistical Machine Translation (SMT).
Since this is dilTerent in Spanish and Catalan, we modify the word order in English to make it more similar to the Spanish/Catalan one and to help the verb treatment mentioned above.
(NieBen and Ney, 200 Ib) introduce hier­ archical lexicon models including baseform and POS information for translation from German into English.
(NieBen and Ney, 200 l a) propose reordering oper­ ations for the language pair GennanEnglish that help SMT by harmonizing word order between source and target.
In the work pre­ sented here, we restrict ourselves to transfotming only one language of the two: the source, which has the less inflected morphology.
For descriptions of SMT systems see for exam­ ple (Germann et al., 2001; Och et al., 1999; Till­ mann and Ney, 2002; Vogel et al..
2000; Wang and Waibel, 1997).
Our experience on several corpora shows that the error rate or a translation from English into morpholog­ ically richer languages decreases by 10% relative if we aim at producing only the correct baseform instead of the fully inflected word.
This process is rela tively difficult for the algorithm and causes noise in the statistical lexicon i C English pronouns are re­ garded as translations of Spanish or Catalan verbs.
Table 6 shows several sentences from the English LC-STAR develop and test corpus that were trans 4 The lemmatization of Spanish and CaLalan was produced using the analyser from UPC Barcelona: MACO+ andRE­ LAX.
Using POS tags as additional knowledge source, we enrich the English verbs such that they contain more information relevant for selecting the conect inflected form in the target language.
</font>
    </div></td>
    <td>&nbsp;
    <button type="button" class="btn btn-info" data-toggle="collapse" data-target="#demo39">LUNH</button>
        <div id="demo39" class="collapse"><font color="black">
    One of the main problems when translat­ ing from a language with hm·dly any inflectional morphology (which is English in our experiments) into one with richer morphology (here: Spanish and Catalan) is the production of the correct in­ flected form in the target lan guage.
We introduce transformations to the English string that are based on the part-of-speech information and show how this knowledge source can help SMT.
Unlike this, we investigate methods for enriching English with knowledge to help selecting the correct fullform in a morphologically richer language.
(GarciaVarea et al..
2001) apply a maximum en­ tropy approach for training the statistical lexicon, but do not take any linguistic information into ac­ count.
If necessary, the inverse of these transformations will be applied to the generated output string.
In the work pre­ sented here, we restrict ourselves to transfotming only one language of the two: the source, which has the less inflected morphology.
Our experience on several corpora shows that the error rate or a translation from English into morpholog­ ically richer languages decreases by 10% relative if we aim at producing only the correct baseform instead of the fully inflected word.
This process is rela tively difficult for the algorithm and causes noise in the statistical lexicon i C English pronouns are re­ garded as translations of Spanish or Catalan verbs.
We presented a method for improving quality of statistical machine translation from English into morphologically richer languages like Spanish and Catalan.
Using POS tags as additional knowledge source, we enrich the English verbs such that they contain more information relevant for selecting the conect inflected form in the target language.
</font>
    </div></td>
    <td>&nbsp;
    <button type="button" class="btn btn-info" data-toggle="collapse" data-target="#demo49">PGN</button>
        <div id="demo49" class="collapse"><font color="black">
    the transfonnations we apply arc the following : treatment of verbs in catalan and spanish , the pronoun before a verb is often omitted and in stead .
we introduce transformations to the english string that we apply to the less inflected language of the two under consideration -lrb- namely english -rrb- in section 4 .
then , we conclude and present ideas about future work in section 7 .
information contained in the german en tries that are relevant for the generation of the english translation arc omitted .
the question inversion we apply was inspired by this ; nevertheless , we do not per form a full morpho-syntactic analysis , it make use only of pos information which can be ob tained from freely available tools .
2001 -rrb- apply a maximum en tropy approach for training the statistical lexicon do not take any linguistic information into ac count .
deci sion rule yields the following criterion : arg max pr -lrb- t -lcb- l l -lcb- -rrb- · pr -lrb- s -lcb- l l -lcb- -rrb- -rcb- -lrb- 1 -rrb- tl through this decomposition of probability , we obtain two knowledge sources : the translation and the language model .
the probability of a target language word to occur in the target string is assumed to depend basically only on the source words aligned to it
our experience on several corpora shows that the error rate or a translation from english into morpholog ically richer languages decreases by 10 % relative if we aim it ` crees ' obtain the information needed to select the con · ect verb form in the target language from one single english word get .
for example , we splice the phrase ` you think ' to form the single entry ` you _ think ' which con tains sufficient information .
training except fi · ' -lrb- or one of its inflections -rrb- has never been seen in the lexicon and verbs as de scribed above , it might happen thalthe verh itsell ' -lrb- or ds do yo u _ di d yo u _ ha ve .
ta gs spl ice d w or ds as described in seclions 4.1 and 4.2 • train the statistical translation system using this modi fled source language corpus 1 • with the resuiling alignment .
</font>
    </div></td>
    </tr>
    </table>
        </div>
</div>

</div></div></body></html>